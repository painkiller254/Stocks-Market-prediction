{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API_KEY = \"QAUD1J9NCAL8HVR3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\badbw\\OneDrive\\Desktop\\Stock Prices\\Time_series.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/badbw/OneDrive/Desktop/Stock%20Prices/Time_series.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_to_save)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/badbw/OneDrive/Desktop/Stock%20Prices/Time_series.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/badbw/OneDrive/Desktop/Stock%20Prices/Time_series.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# ====================== Loading Data from kaggle ==================\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/badbw/OneDrive/Desktop/Stock%20Prices/Time_series.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# data normalization\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/badbw/OneDrive/Desktop/Stock%20Prices/Time_series.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mStocks\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhqs.us.txt\u001b[39m\u001b[39m'\u001b[39m),delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, usecols\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOpen\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHigh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data_source ='kaggle'\n",
    "\n",
    "if data_source == 'alphavantage':\n",
    "    api_key = 'QAUD1J9NCAL8HVR3'\n",
    "\n",
    "    #America Airlines stock market prices\n",
    "    ticker = \"ALL\"\n",
    "\n",
    "    url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=%s&outputsize=full&apikey=%s\"%(ticker,api_key)\n",
    "\n",
    "    file_to_save = 'stock_market_data-%s.csv'%ticker\n",
    "\n",
    "    if not os.path.exists(file_to_save):\n",
    "        with urllib.request.urlopen(url_string) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            data = data['Time Series (Daily)']\n",
    "            # extract stock maret data\n",
    "            data = data['Time Series (Daily)']\n",
    "            df = pd.DataFrame(columns=['Date', 'Low', 'High', 'Close', 'Open'])\n",
    "            for k, v in data.items():\n",
    "                date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "                data_row = [date.date(),float(v['3. low']), float(v['2. high']), float(['4. close']), float(['1. open'])]\n",
    "                df.loc[-1,:] = data_row\n",
    "                df.index = df.index +1\n",
    "            print('Data saved to : %s'%file_to_save)\n",
    "            df.to_csv(file_to_save)\n",
    "\n",
    "    else:\n",
    "        print('File already exists. Loading data from CSV')\n",
    "        df = pd.read_csv(file_to_save)\n",
    "\n",
    "else:\n",
    "    # ====================== Loading Data from kaggle ==================\n",
    "    # data normalization\n",
    "    df = pd.read_csv(os.path.join('Stocks', 'hqs.us.txt'),delimiter=',', usecols=['Date', 'Open', 'High', 'Low', 'Close'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by date\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Double check the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)\n",
    "plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500], rotation=45)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Mid Price', fontsize=18)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data into a Training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First alculate the mid prices from the highest and lowest\n",
    "high_prices = df.loc[:,'High'].as_matrix()\n",
    "low_prices = df.loc[:,'Low'].as_mtrix()\n",
    "mid_prices = (high_prices+low_prices)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mid_prices[:11000]\n",
    "test_data = mid_prices[11000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to be between 0 and 1\n",
    "# normalize both test and train with re\n",
    "from audioop import minmax\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshaoe(-1,1)\n",
    "test_data = test_data.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Scaler with training data and smooth data\n",
    "smoothing_window_size = 2500\n",
    "for di in range(0, 10000, smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
    "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
    "\n",
    "# You normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
    "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform exponential moving average smoothing\n",
    "# so the data will have a smoother curve than the original ragged data\n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "for ti in range(11000):\n",
    "    EMA = gamma*train_data[ti] + (1-gamma)*EMA\n",
    "    train_data[ti] = EMA\n",
    "\n",
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data, test_data],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Step Ahead Prediction via Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "std_avg_predictions = []\n",
    "std_avg_x = []\n",
    "mse_errors = []\n",
    "\n",
    "for pred_idx in range(window_size,N):\n",
    "    if pred_idx >= N:\n",
    "        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
    "    else:\n",
    "        date = df.loc[pred_idx, 'Date']\n",
    "\n",
    "    std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n",
    "    mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    std_avg_x.append(date)\n",
    "\n",
    "print('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "\n",
    "\n",
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]), all_mid_data,color='b',label='True')\n",
    "plt.plot(range(window_size,N),std_avg_predictions,color='orange',label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "\n",
    "run_avg_predictions = []\n",
    "run_avg_x = []\n",
    "\n",
    "mse_errors = []\n",
    "\n",
    "running_mean = 0.0\n",
    "run_avg_predictions.append(running_mean)\n",
    "\n",
    "decay = 0.5\n",
    "\n",
    "for pred_idx in range(1,N):\n",
    "    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n",
    "    run_avg_predictions.append(running_mean)\n",
    "    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    run_avg_x.append(date)\n",
    "\n",
    "print('MSE error for EMA averagingL %.5f'%(0.5*np.mean(mse_errors)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),all_mid_data, color='b',label=\"True\")\n",
    "plt.plot(range(0,N),run_avg_predictions,color='orange', lable='Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro to LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorSeq(object):\n",
    "\n",
    "    def __init__(self, prices, batch_size, num_unroll):\n",
    "        self._prices = prices\n",
    "        self._prices_length = len(self._prices) - num_unroll\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._segments = self._prices_length //self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._bath_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),stype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            if self._cursor[b]+1>=self._prices_length:\n",
    "                # self._cursor[b] = b*self._segments\n",
    "                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n",
    "\n",
    "            batch_data[b] = self._prices[self._cursor[b]]\n",
    "            batch_labels[b] = self._pries[self._cursor[b]+np.random.randint(0,5)]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "\n",
    "    def unroll_batches(self):\n",
    "\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        init_data, init_label = None,None\n",
    "        for ui in range(self._num_unroll):\n",
    "\n",
    "            data, labels = self.next_batch()\n",
    "\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indeices(self):\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n",
    "\n",
    "\n",
    "\n",
    "dg = DataGeneratorSeq(train_data,5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):\n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = lbl\n",
    "    print('\\tInputs: ',dat)\n",
    "    print('\\n\\tOutput',lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1 # Dimensionality of the data. \n",
    "num_unrollings = 50 # Number of time steps you look into the future\n",
    "batch_size = 500 # Number of samples in a batch\n",
    "num_nodes = [200,100,150] # Number of hidden nodes in each layer of the deep LSTM stack we're using\n",
    "n_layers = len(num_nodes) # number of layers\n",
    "dropout = 0.2 # dropout amount\n",
    "\n",
    "tf.reset_default_graph() # This is important in case you run this multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "train_inputs, train_outputs = [], []\n",
    "\n",
    "# You unroll the input over time defining placeolders for each time step\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D], name='train_inputs_%d'%ui))\n",
    "    train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,1], name='train_outputs_%d'%ui))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Parameters of the LSTM and Regression layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cells = [\n",
    "    tf.ontrib.rnn.LSTMCell(num_units=num_nodes[li], state_is_tuple=True, initializer=tf.contrib.layers.xavier_initializer())\n",
    "for li in range(n_layers)\n",
    "]\n",
    "\n",
    "drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(\n",
    "    lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout\n",
    ")for lstm in lstm_cells]\n",
    "drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "\n",
    "w = tf.get_variable('W',shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b', initializer=tf.random_uniform([1], -0.1,0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating LSTM output and Feeding it to the regression layer to get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cell state and hidden state variables to maintain the state of the LSTM\n",
    "c, h = [], []\n",
    "initial_state = []\n",
    "for li in range(n_layers):\n",
    "    c.append(tf.Variables(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "    h.append(tf.Variables(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "    initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n",
    "# Do several tensor transofmations, because the function dynamic_rnn requires the output to be of a specific format. \n",
    "all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs], axis=0)\n",
    "\n",
    "# all_ouputs is [seq_length, batch_size, num_nodes]\n",
    "all_lstm_outputs, state = tf.nn.dynamic_rnn(\n",
    "    drop_multi_cell, all_inputs, initial_state=tuple(initial_state), \n",
    "    time_major = True, dtype=tf.float32\n",
    ")\n",
    "\n",
    "all_lstm_outputs = tf.reshape(all_lstm_outputs, batch_size*num_unrollings,num_nodes[-1])\n",
    "\n",
    "all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)\n",
    "\n",
    "split_outputs = tf.split(all_outputs, num_unrollings, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Calulation and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When calculating the loss you need to be careful about the exact form, because you calculate\n",
    "# loss of all the unrolled steps at the same time\n",
    "# Therefore, take the mean error or each batch and get the sum of that over all the unrolled steps\n",
    "\n",
    "print('Defining trainig Loss')\n",
    "loss = 0.0\n",
    "with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)] + [tf.assign(h[li], state[li][1]) for li in range(n_layers]):\n",
    "    for ui in range(num_unrollings):\n",
    "        loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n",
    "    \n",
    "print('Learning rate decay operations')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "tf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "tf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "\n",
    "learning_rate = tf.maximum(\n",
    "    tf.train.exponential_decay(tf_learning_rate, global_step, decaysteps=1, decay_rate=0.5, staircase=True),\n",
    "    tf_min_learning_rate)\n",
    "\n",
    "#Optimizer\n",
    "print('TF Optimization operations')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.ompute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))\n",
    "\n",
    "print('\\tAll done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predition related calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DEfining prediction related TF functions')\n",
    "\n",
    "sample_inputs = tf.placeholder(tf.float32, shape=[1,D])\n",
    "\n",
    "# Maintaining LSTM state for prediction stage\n",
    "sample_c, sample_h, initial_sample_state = [],[],[]\n",
    "for li in range(n_layers):\n",
    "    sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "    sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "    initial_sample_state.append(tf.ontrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))\n",
    "\n",
    "reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)], *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n",
    "\n",
    "sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0), initial_state=tuple(initial_sample_state), time_major = True, dtype=tf.float32)\n",
    "\n",
    "with tf.control_dependencies([tf.assign(sample_c[li], sample_state[li][0]) for li in range(n_layers)] + [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):\n",
    "    sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "valid_summary = 1 # interval you make test predictions\n",
    "\n",
    "n_predict_once = 50 # Number of steps you continously predict for\n",
    "\n",
    "train_seq_length = train_data.size # Full length of the training data\n",
    "\n",
    "train_mse_ot = [] # Accumulate Train losses\n",
    "test_mse_ot = [] # Accumulate Test loss\n",
    "predictions_over_time = [] # Accumulate predictions\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Used for decaying learning rate\n",
    "loss_nondecrease_count = 0\n",
    "loss_nondecrease_threshold = 2 # If the test error hasn't increased in this many steps, decrease the learning rate\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# Define data generator\n",
    "data_gen = DataGeneratorSeq(train_data, batch_size, num_unrollings)\n",
    "\n",
    "x_axis_seq = []\n",
    "\n",
    "# Points you start your test predictions from\n",
    "test_points_seq = np.arange(11000,12000,50).tolist()\n",
    "\n",
    "for ep in range(epochs):\n",
    "\n",
    "    # =========================== Training ============================\n",
    "    for step in range(train_seq_length//batch_size):\n",
    "\n",
    "        u_data, u_labels = data_gen.unroll_batches()\n",
    "\n",
    "        feed_dict = {}\n",
    "        for ui,(dat, lbl) in enumerate(zip(u_data,u_labels)):\n",
    "            feed_dict[train_inputs[ui]] = dat.reshape(-1,1)\n",
    "            feed_dict [train_outputs[ui]] = lbl.reshape(-1,1)\n",
    "\n",
    "        feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate: 0.000001})\n",
    "\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "        average_loss += 1\n",
    "\n",
    "        # ========================= Validation ===================================\n",
    "        if (ep+1) % valid_summary == 0:\n",
    "\n",
    "            average_loss = average_loss/(valid_summary*(train_seq_length//batch_size))\n",
    "\n",
    "            # The average loss\n",
    "            if (ep+1%)%valid_summary ==0:\n",
    "                print('Average loss at step %d: %f' % (ep+1, average_loss))\n",
    "\n",
    "            train_mse_ot.append(average_loss)\n",
    "\n",
    "            average_loss = 0 # reset loss\n",
    "            \n",
    "            presictions_seq = []\n",
    "\n",
    "            mse_test_loss_seq = []\n",
    "\n",
    "            # ================== Updating State and Making Predictions ========================\n",
    "            for w_i in test_points_seq:\n",
    "                mse_test_loss = 0.0\n",
    "                our_predictions = []\n",
    "\n",
    "                if (ep+1)-valid_summary==0:\n",
    "                    # Only calculate x_axis values in the first validation epoch\n",
    "                    x_axis=[]\n",
    "\n",
    "                # Feed in the recent past behaviour of stock prices\n",
    "                # to make predictions from that point onwards\n",
    "                for tr_i in range(w_i-num_unrollings+1,w_i-1):\n",
    "                    current_price = all_mid_data[tr_i]\n",
    "                    feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)\n",
    "                    _ = session.run(sample_prediction, fees_dict=feed_dict)\n",
    "\n",
    "                feed_dict = {}\n",
    "\n",
    "                current_price = all_mid_data[w_i-1]\n",
    "\n",
    "                feed_dict[sample_inputs] = np,array(current_price).reshape(1,1)\n",
    "\n",
    "                # Make predictiions for this many steps\n",
    "                # Each prediction uses previous prediction as it's current input\n",
    "                for pred_i in range(n_predict_once):\n",
    "\n",
    "                    pred = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "                    our_predictions.append(np.asscalar(pred))\n",
    "\n",
    "                    feed_dict[sample_inputs] = np.asarray(pred).reshape(-1,1)\n",
    "\n",
    "                    if (ep+1)-valid_summary == 0:\n",
    "                        # Only calculate x_axis values in the first validation epoch\n",
    "                        x_axis.append(w_i+pred_i)\n",
    "\n",
    "                    mse_test_loss += 0.5*(pred-all_mid_data[w_i+pred_i]) ** 2\n",
    "                \n",
    "                session.run(reset_sample_states)\n",
    "\n",
    "                predictions_seq.append(np.array(our_predictions))\n",
    "\n",
    "                mse_test_loss /= n_predict_once\n",
    "                mse_test_loss_seq.append(mse_test_loss)\n",
    "\n",
    "                if (ep+1)-valid_summary==0:\n",
    "                    x_axis_seq.append(x_axis)\n",
    "\n",
    "            current_test_mse = np.mean(mse_test_loss_seq)\n",
    "\n",
    "            # learning rate decay logic\n",
    "            if len(test_mse_ot)>0 and current_test_mse > min(test_mse_ot):\n",
    "                loss_nondecrease_count += 1\n",
    "            else:\n",
    "                loss_nondecrease_count = 0\n",
    "\n",
    "            if loss_nondecrease_count > loss_nondecrease_threshold :\n",
    "                session.run(inc_gstep)\n",
    "                loss_nondecrease_count = 0\n",
    "                print('\\tDecreasing learning rate by 0.5')\n",
    "\n",
    "            test_mse_ot.append(current_test_mse)\n",
    "            print('\\tTest MSE: %.5f'%np.mean(mse_test_loss_seq))\n",
    "            predictions_over_time.append(predictions_seq)\n",
    "            print('\\tFinished Predictions')\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prediction_epoch = 28\n",
    "\n",
    "plt.figure(figsize = (18,18))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
    "\n",
    "# plotting how the prediction changes over time\n",
    "# Plot older predictions with low alpha and newer predictions with high alpha\n",
    "start_alpha = 0.25\n",
    "alpha = np.arange(start_alpha,1.1,(1.0-start_alpha)/len(predictions_over_time[::3]))\n",
    "for p_i,p in enumerate(predictions_over_time[::3]):\n",
    "    for xval,yval in zip(x_axis_seq,p):\n",
    "        plt.plot(xval,yval,color='r',alpha=alpha[p_i])\n",
    "\n",
    "plt.title('Evolution of Test Predictions Over Time', fontsize=18)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Mid Price', fontsize=18)\n",
    "plt.xlim(11000, 12500)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "# Predicting the best test prediction you got\n",
    "plt.plot(range(df.shape[0]), all_mid_data,color='b')\n",
    "for xval,yval in zip(x_axis_seq, predictions_over_time[best_prediction_epoch]):\n",
    "    plt.plot(xval,yval,color='r')\n",
    "\n",
    "plt.title('Best Test Predictions Over Time', fontsize=18)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Mid Price', fontsize=18)\n",
    "plt.xlim(11000, 12500)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/tutorial/lstm-python-stock-market"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5fb38b81ae13b7255ba059d3b33d725c32e55336956a8ad013abd09efed46c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
